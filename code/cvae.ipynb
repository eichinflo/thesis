{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE) on dummy-data\n",
    "A first try to work with the data provided by Caroline Broichhagen (broichha@imbi.uni-freiburg.de). Ultimate goal of this kernel is going to be a simple Variational Autoencoder with one-layer convolutional encoder and decoder to reconstruct the data.\n",
    "\n",
    "First, let's import the data and have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported."
     ]
    }
   ],
   "source": [
    "using MAT, Plots, ColorSchemes\n",
    "\n",
    "# by broichha@imbi.uni-freiburg.de\n",
    "file = matopen(\"/home/flo/projects/thesis/dummydata/dummyData2000.mat\")\n",
    "data = read(file, \"pix3D\")\n",
    "close(file)\n",
    "\n",
    "# store data in WHCN order (width, height, channel, batches)\n",
    "data = reshape(data, (60, 60, 1, size(data, 3)))\n",
    "print(\"Data imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: (60, 60, 1, 2000)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Saved animation to \n",
      "│   fn = /home/flo/projects/thesis/code/in.gif\n",
      "└ @ Plots /home/flo/.julia/packages/Plots/Iuc9S/src/animation.jl:95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"in.gif\" />"
      ],
      "text/plain": [
       "Plots.AnimatedGif(\"/home/flo/projects/thesis/code/in.gif\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Data dimensions: \", size(data))\n",
    "\n",
    "# by broichha@imbi.uni-freiburg.de\n",
    "anim = @animate for i=1:size(data, 4)\n",
    "    Plots.heatmap(data[:, :, 1, i], seriescolor=cgrad(ColorSchemes.gray.colors))\n",
    "end\n",
    "gif(anim, \"in.gif\", fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy data consists of 2000 60x60 images that simulate the activity represented in the above gif. For now, we'll just challenge my sparse Julia abilities to build a VAE to reconstruct the above data. We will later on attempt to use the latent representation learned by our VAE to find regions of interest (ROIs) in the single images of the dataset and discriminate from background activity and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: @epochs, mse\n",
    "using Base.Iterators: partition\n",
    "using Images\n",
    "using Flux: Conv, MaxPool, Dense, ConvTranspose\n",
    "using BSON: @save, @load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# TODO: all params over here\n",
    "changed = false; # set to true, if model parameters were changed\n",
    "latent_dimension = 15;\n",
    "epochs = 50;\n",
    "out_ch1 = 8;\n",
    "learning_rate = 0.001;\n",
    "batch_size = 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer of our encoder will be a convolutional one. For debugging purposes we output intermediate sizes of data tensors. Good hyperparameters still up to be tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of output at layer1: (60, 60, 8, 10)\n",
      "Size of output after max pooling: (20, 20, 8, 10)"
     ]
    }
   ],
   "source": [
    "layer1 = Conv((3, 3), 1=>out_ch1, relu, pad=1);\n",
    "sample1 = layer1(data[:, :, :, 1:10])\n",
    "print(\"Size of output at layer1: \", size(sample1), \"\\n\");\n",
    "\n",
    "pool1 = MaxPool((3, 3));\n",
    "sample2 = pool1(sample1)\n",
    "out_size = size(sample2)\n",
    "print(\"Size of output after max pooling: \", out_size)\n",
    "\n",
    "conv1(x) = pool1(layer1(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will flatten the output tensor and input it into a fully connected layer. Again, we keep track of dimensions to avoid mistakes. Maybe we should find a better solution than inputting data samples for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of output at layer2: (15, 10)"
     ]
    }
   ],
   "source": [
    "flattened_size = out_size[1] * out_size[2] * out_size[3];\n",
    "layer2 = Dense(flattened_size, latent_dimension, relu);\n",
    "sample3 = layer2(reshape(sample2, (flattened_size, 10)));\n",
    "print(\"Size of output at layer2: \", size(sample3))\n",
    "\n",
    "encoder(x) = layer2(reshape(conv1(x), (flattened_size, :)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our z variable, the very heart of our VAE. In order to maintain the ability to use backpropagation for parameter optimization, we make use of a reparametrization trick where we split z into mean, variance and a random part epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reparametrization trick in vae\n",
    "function sampling(mean, variance)\n",
    "    epsilon = rand(latent_dimension)\n",
    "    return mean .+ exp.(0.5 .* variance) .* epsilon\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of z: (15, 10)"
     ]
    }
   ],
   "source": [
    "z_mean = Dense(latent_dimension, latent_dimension, relu)\n",
    "z_log_var = Dense(latent_dimension, latent_dimension, relu)\n",
    "z(x) = sampling(z_mean(x), z_log_var(x))\n",
    "\n",
    "sample4 = z(sample3)\n",
    "print(\"Size of z: \", size(sample4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decoder, for now, we just use a fully connected layer followed by a deconvolutional layer. Not sure, if the output layer should be another fully connected one with sigmoid activation. For now, we'll leave it like this and improve later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of output at layer 4: (60, 60, 8, 10)\n",
      "Size of output at layer 5: (60, 60, 1, 10)"
     ]
    }
   ],
   "source": [
    "# decoder\n",
    "layer3 = Dense(latent_dimension, 400, relu)\n",
    "\n",
    "layer4 = ConvTranspose((3, 3), 1=>out_ch1, relu, stride=3)\n",
    "sample5 = layer4(reshape(layer3(sample4), (20, 20, 1, 10)))\n",
    "print(\"Size of output at layer 4: \", size(sample5), \"\\n\")\n",
    "\n",
    "layer5 = ConvTranspose((3, 3), out_ch1=>1, relu, pad=1);\n",
    "sample6 = layer5(sample5);\n",
    "print(\"Size of output at layer 5: \", size(sample6))\n",
    "\n",
    "decoder(x) = layer5(layer4(reshape(layer3(x), (20, 20, 1, :))));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine encoder, latent variable z and decoder and train the model afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae(x) = decoder(z(encoder(x)));\n",
    "\n",
    "loss(x) = mse(cvae(x), x);\n",
    "\n",
    "optimizer = Flux.ADAM(learning_rate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 1\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 2\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 3\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 4\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 5\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 6\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 7\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 8\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 9\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 10\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 11\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 12\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 13\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 14\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 15\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 16\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 17\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 18\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 19\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 20\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 21\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 22\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 23\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 24\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 25\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 26\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 27\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 28\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 29\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 30\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 31\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 32\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 33\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 34\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 35\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 36\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 37\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 38\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 39\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 40\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 41\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 42\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 43\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 44\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 45\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 46\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 47\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 48\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 49\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n",
      "┌ Info: Epoch 50\n",
      "└ @ Main /home/flo/.julia/packages/Flux/dkJUV/src/optimise/train.jl:105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done."
     ]
    }
   ],
   "source": [
    "# for some weird reason ∇maxpool has a problem with Float64 data\n",
    "# as dy seems to be computed in Float32 and the function takes\n",
    "# only arrays with values of same type\n",
    "data = convert.(Float32, data);\n",
    "\n",
    "# this also looks a bit hacked, but it does the job of bringing the \n",
    "# data into the shape preferred by Flux.train! and creating batches\n",
    "# (we'll find a better solution)\n",
    "batches = [reshape(data[:, :, :, ((i-1)*batch_size+1):(i*batch_size)], (60, 60, 1, batch_size)) for i in 1:size(data, 4)÷batch_size];\n",
    "\n",
    "# load previous parameters, if existing\n",
    "if isfile(\"cvae_model.bson\") && !changed\n",
    "    @load \"cvae_model.bson\" cvae\n",
    "end\n",
    "\n",
    "print(\"Training...\")\n",
    "@epochs epochs Flux.train!(loss, params(cvae), zip(batches), optimizer)\n",
    "print(\"done.\")\n",
    "# store model for later\n",
    "@save \"cvae_model.bson\" cvae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code seems to be running through. Time to see, how well our model does at reconstructing our dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Saved animation to \n",
      "│   fn = /home/flo/projects/thesis/code/out.gif\n",
      "└ @ Plots /home/flo/.julia/packages/Plots/Iuc9S/src/animation.jl:95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"out.gif\" />"
      ],
      "text/plain": [
       "Plots.AnimatedGif(\"/home/flo/projects/thesis/code/out.gif\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = [reshape(data[:, :, :, i], (60, 60, 1, 1)) for i in 1:size(data, 4)];\n",
    "anim1 = @animate for i=1:2000\n",
    "    output = cvae(sequence[i])\n",
    "    output = reshape(Flux.Tracker.data(output), 60, 60)\n",
    "    Plots.heatmap(output, seriescolor=cgrad(ColorSchemes.gray.colors))\n",
    "end;\n",
    "\n",
    "gif(anim1, \"out.gif\", fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we're not doing well so far. Even without looking at our metric, we can see, that our reconstructed images are bad. We can identify the following reasons for this:\n",
    "\n",
    "- our model has so far only been trained on very few epochs as we did not have access to any high power machines\n",
    "- we did not do any tweaking of the model architecture and hyperparameters\n",
    "- the author is not sure, if the decoder is implemented correctly\n",
    "\n",
    "TODO: \n",
    "- run code on server and enable gpu support (https://fluxml.ai/Flux.jl/stable/gpu/)\n",
    "- test different architectures for encoder and decoder\n",
    "- run on bigger training set and run grid search for good parameters\n",
    "- set up dual-encoder-decoder architecture\n",
    "- think about decoder structure\n",
    "\n",
    "Just some debugging down here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining to plot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Saved animation to \n",
      "│   fn = /home/flo/projects/thesis/code/tmp.gif\n",
      "└ @ Plots /home/flo/.julia/packages/Plots/Iuc9S/src/animation.jl:95\n"
     ]
    }
   ],
   "source": [
    "print(\"Begining to plot...\\n\")\n",
    "@gif for i=1:10\n",
    "    output = cvae(sequence[i])\n",
    "    output = reshape(Flux.Tracker.data(output), 60, 60)\n",
    "    Plots.heatmap(output, seriescolor=cgrad(ColorSchemes.gray.colors))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 60, 1, 2000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 60, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = [data[:, :, :, i] for i in 1:size(data, 4)]\n",
    "size(sequence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5358083559526245"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60×60×1×2000 Array{Float64,4}:\n",
       "[:, :, 1, 1] =\n",
       " 0.424376  0.540171  0.646003  0.696327  …  0.504878  0.487778  0.649834\n",
       " 0.535808  0.43302   0.684178  0.487076     0.480037  0.401764  0.430195\n",
       " 0.564302  0.561957  0.479192  0.502272     0.671898  0.486677  0.460516\n",
       " 0.42173   0.354076  0.409455  0.533954     0.309892  0.544167  0.467044\n",
       " 0.492546  0.391326  0.520655  0.453911     1.36031   0.389626  0.591512\n",
       " 0.611339  0.629831  0.515894  0.664472  …  1.21283   0.439241  0.616249\n",
       " 0.340172  0.406445  0.431677  0.510153     1.27216   0.333637  0.492283\n",
       " 0.528009  0.52316   0.461787  0.506869     1.36426   0.474291  0.550353\n",
       " 0.440486  0.37651   0.523665  0.637959     1.43239   0.655869  0.4776  \n",
       " 0.684479  0.495542  0.405739  0.482995     1.50989   0.451748  0.358121\n",
       " 0.528378  0.692808  0.65008   0.444677  …  0.328437  0.643109  0.46511 \n",
       " 0.422358  0.626238  0.493982  0.552668     0.305274  0.696971  0.593037\n",
       " 0.377155  0.562758  0.366504  0.388486     0.484221  0.541331  0.35749 \n",
       " ⋮                                       ⋱                              \n",
       " 0.619303  0.451257  0.528584  0.60852      0.534437  0.315577  0.530105\n",
       " 0.358822  0.485959  0.300554  0.582651     0.561253  0.692173  0.669494\n",
       " 0.657423  0.589608  0.319273  0.504005  …  0.398257  0.64523   0.482142\n",
       " 0.630702  0.599828  0.455339  0.350253     0.450997  0.580311  0.571041\n",
       " 0.340618  0.318646  0.59837   0.359887     0.453088  0.375478  0.69995 \n",
       " 0.52167   0.591793  0.477651  0.545827     0.459491  0.579559  0.364197\n",
       " 0.510907  0.673332  0.69453   0.431616     0.404575  0.374134  0.348601\n",
       " 0.613143  0.553345  0.658746  0.509823  …  0.394457  0.425602  0.399774\n",
       " 0.443968  0.404792  0.566517  0.497622     0.601033  0.493085  0.501584\n",
       " 0.682989  0.443654  0.587341  0.490601     0.404842  0.643147  0.300578\n",
       " 0.680345  0.439374  0.39314   0.523789     0.551288  0.508096  0.562091\n",
       " 0.347701  0.622458  0.377278  0.321778     0.491655  0.323103  0.521863\n",
       "\n",
       "[:, :, 1, 2] =\n",
       " 0.340067  0.494665  0.457239  0.616856  …  0.328399  0.594935  0.323925\n",
       " 0.372322  0.459353  0.689861  0.558259     0.321457  0.640994  0.48103 \n",
       " 0.412933  0.367783  0.596589  0.584084     0.63584   0.684652  0.420123\n",
       " 0.651981  0.578174  0.514395  0.495447     0.399979  0.52276   0.636803\n",
       " 0.673696  0.59054   0.425212  0.372731     1.26018   0.640727  0.649383\n",
       " 0.507572  0.445355  0.655261  0.339522  …  1.27745   0.637726  0.438164\n",
       " 0.569707  0.544164  0.320376  0.44788      1.32013   0.333769  0.66147 \n",
       " 0.334705  0.490746  0.412586  0.421306     1.4842    0.565896  0.693827\n",
       " 0.692256  0.490601  0.581314  0.605873     1.35515   0.629138  0.547007\n",
       " 0.452515  0.484765  0.478694  0.393784     1.59861   0.678101  0.430873\n",
       " 0.582852  0.560319  0.660065  0.400449  …  0.407368  0.651741  0.675115\n",
       " 0.435558  0.669162  0.406537  0.453233     0.414313  0.661524  0.57714 \n",
       " 0.619804  0.608481  0.657872  0.497253     0.609493  0.689179  0.636393\n",
       " ⋮                                       ⋱                              \n",
       " 0.590526  0.487966  0.449987  0.522924     0.4214    0.506397  0.689681\n",
       " 0.389174  0.412916  0.54982   0.354183     0.381919  0.431542  0.425218\n",
       " 0.364897  0.593887  0.591808  0.516452  …  0.530282  0.533333  0.379155\n",
       " 0.682821  0.530828  0.462948  0.312819     0.616634  0.512062  0.564509\n",
       " 0.441042  0.421046  0.69704   0.631543     0.583145  0.672983  0.451931\n",
       " 0.451364  0.666644  0.379301  0.366363     0.488575  0.46883   0.617988\n",
       " 0.30318   0.612076  0.321688  0.564663     0.486247  0.374296  0.625377\n",
       " 0.407717  0.395086  0.452012  0.567828  …  0.359575  0.639894  0.374384\n",
       " 0.420485  0.69818   0.491633  0.439141     0.661947  0.627853  0.392259\n",
       " 0.398982  0.43684   0.42505   0.398606     0.369353  0.546727  0.511389\n",
       " 0.530694  0.515148  0.425199  0.552522     0.452377  0.339267  0.318602\n",
       " 0.577572  0.478448  0.47554   0.42601      0.588327  0.319493  0.571779\n",
       "\n",
       "[:, :, 1, 3] =\n",
       " 0.325664  0.317794  0.314177  0.640204  …  0.664613  0.327717  0.503096\n",
       " 0.323895  0.64256   0.529644  0.365726     0.672476  0.366239  0.421181\n",
       " 0.573506  0.394179  0.572631  0.54216      0.674681  0.532754  0.476507\n",
       " 0.32604   0.53753   0.556802  0.557491     0.328837  0.635756  0.381269\n",
       " 0.607743  0.632573  0.401332  0.372723     1.43143   0.666531  0.609154\n",
       " 0.56212   0.365263  0.435119  0.565478  …  1.30386   0.352549  0.453629\n",
       " 0.494402  0.494556  0.530189  0.698073     1.21741   0.538005  0.64792 \n",
       " 0.553356  0.500767  0.639493  0.548115     1.42157   0.572374  0.359469\n",
       " 0.569387  0.62368   0.539521  0.301562     1.32741   0.302852  0.469105\n",
       " 0.461907  0.344409  0.34677   0.327179     1.40943   0.361213  0.326997\n",
       " 0.415704  0.593104  0.3226    0.377764  …  0.455781  0.493359  0.603217\n",
       " 0.473515  0.436269  0.548709  0.330944     0.316031  0.343273  0.635524\n",
       " 0.41519   0.486357  0.448022  0.550378     0.656484  0.536766  0.358319\n",
       " ⋮                                       ⋱                              \n",
       " 0.678297  0.367202  0.357656  0.609281     0.536219  0.322252  0.5016  \n",
       " 0.483326  0.417147  0.424048  0.380875     0.550967  0.425676  0.48881 \n",
       " 0.333468  0.327557  0.421748  0.530965  …  0.68322   0.624322  0.30526 \n",
       " 0.471112  0.490948  0.541892  0.621194     0.492727  0.502788  0.601244\n",
       " 0.369936  0.314791  0.58676   0.584667     0.647282  0.666796  0.34126 \n",
       " 0.45952   0.352647  0.497542  0.561189     0.530556  0.338467  0.601944\n",
       " 0.52573   0.392222  0.48849   0.330977     0.329192  0.341099  0.4198  \n",
       " 0.60852   0.39044   0.620515  0.477966  …  0.363141  0.595788  0.565274\n",
       " 0.527758  0.590909  0.667199  0.670112     0.462432  0.314242  0.462953\n",
       " 0.463966  0.625229  0.503384  0.520619     0.626957  0.6859    0.313127\n",
       " 0.400669  0.699854  0.538518  0.443554     0.462528  0.511141  0.372695\n",
       " 0.484695  0.511504  0.376556  0.520425     0.545706  0.623767  0.617315\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 1, 1998] =\n",
       " 0.530218  0.402809  0.503665  0.663379  …  0.650563  0.535453  0.31653 \n",
       " 0.379166  0.596928  0.395117  0.346935     0.690607  0.632027  0.623023\n",
       " 0.425598  0.303383  0.426593  0.460303     0.457856  0.540124  0.651983\n",
       " 0.495115  0.692805  0.362059  0.453901     0.393547  0.477161  0.449669\n",
       " 0.393826  0.437868  0.451243  0.514017     0.501918  0.484303  0.698301\n",
       " 0.600967  0.335669  0.68694   0.316111  …  0.68802   0.674244  0.49099 \n",
       " 0.416866  0.447639  0.549847  0.424581     0.373344  0.353142  0.467278\n",
       " 0.332563  0.597562  0.441366  0.53158      0.363637  0.427261  0.688986\n",
       " 0.592604  0.435116  0.453334  0.699043     0.428969  0.629876  0.493561\n",
       " 0.570817  0.595792  0.516223  0.3424       0.400983  0.634457  0.648504\n",
       " 0.585751  0.520247  0.651522  0.637025  …  0.679703  0.695007  0.365317\n",
       " 0.475079  0.378591  0.596022  0.406669     0.65696   0.56644   0.349217\n",
       " 0.508139  0.440912  0.340297  0.459754     0.326994  0.630134  0.574331\n",
       " ⋮                                       ⋱                              \n",
       " 0.693492  0.652251  0.325916  0.625163     0.455485  0.648513  0.613648\n",
       " 0.440029  0.4856    0.412006  0.416748     0.325764  0.531694  0.359365\n",
       " 0.562724  0.311824  0.561584  0.379536  …  0.561864  0.494482  0.468984\n",
       " 0.344831  0.382656  0.325749  0.521659     0.693614  0.577185  0.513244\n",
       " 0.414187  0.554905  0.520805  0.306751     0.656022  0.395114  0.651769\n",
       " 0.319146  0.641198  0.318133  0.323126     0.39771   0.54908   0.393019\n",
       " 0.588508  0.459544  0.560262  0.630767     0.486156  0.41546   0.525409\n",
       " 0.348937  0.442257  0.647928  0.618822  …  0.494803  0.587906  0.434345\n",
       " 0.346359  0.406432  0.357693  0.30743      0.686512  0.546367  0.349625\n",
       " 0.359144  0.455615  0.361227  0.673862     0.564363  0.532695  0.373225\n",
       " 0.623057  0.434184  0.456824  0.559166     0.508214  0.402098  0.359629\n",
       " 0.59635   0.60582   0.579616  0.311218     0.681054  0.53423   0.411871\n",
       "\n",
       "[:, :, 1, 1999] =\n",
       " 0.429291  0.637283  0.452588  0.487585  …  0.384046  0.335167  0.623211\n",
       " 0.436087  0.349854  0.57901   0.670101     0.331307  0.578004  0.525171\n",
       " 0.362213  0.306058  0.665627  0.679992     0.53074   0.515977  0.506501\n",
       " 0.311611  0.330375  0.448286  0.695391     0.388551  0.579358  0.663707\n",
       " 0.462267  0.436437  0.410015  0.694499     0.670403  0.510594  0.495907\n",
       " 0.478183  0.656604  0.619355  0.389411  …  0.35825   0.606886  0.524923\n",
       " 0.359075  0.472567  0.344406  0.35352      0.638775  0.494087  0.685253\n",
       " 0.58418   0.593344  0.581869  0.358925     0.47883   0.462298  0.699201\n",
       " 0.648611  0.451847  0.465543  0.428697     0.60809   0.359358  0.579304\n",
       " 0.444811  0.57227   0.570376  0.688337     0.564651  0.413585  0.450279\n",
       " 0.5523    0.314529  0.465737  0.618175  …  0.463375  0.683034  0.673313\n",
       " 0.520372  0.66409   0.437294  0.613516     0.530471  0.658334  0.562355\n",
       " 0.559502  0.433263  0.344785  0.563665     0.638583  0.389366  0.685564\n",
       " ⋮                                       ⋱                              \n",
       " 0.397789  0.438739  0.653517  0.431143     0.593713  0.57497   0.599633\n",
       " 0.409648  0.580486  0.366023  0.338155     0.590148  0.399173  0.462803\n",
       " 0.487279  0.663063  0.636811  0.571964  …  0.534402  0.404623  0.516761\n",
       " 0.437528  0.63949   0.559661  0.687035     0.37347   0.357226  0.620745\n",
       " 0.645361  0.55079   0.48821   0.434752     0.509287  0.429842  0.495108\n",
       " 0.5051    0.447319  0.666662  0.54561      0.467839  0.348789  0.465155\n",
       " 0.560558  0.651492  0.446437  0.621634     0.615096  0.32253   0.396546\n",
       " 0.446503  0.667921  0.362408  0.330851  …  0.630765  0.40081   0.48312 \n",
       " 0.621922  0.691954  0.474928  0.698155     0.618125  0.547717  0.568417\n",
       " 0.496519  0.338186  0.627969  0.455589     0.454669  0.529726  0.303962\n",
       " 0.488622  0.653385  0.575422  0.654724     0.524163  0.563156  0.458713\n",
       " 0.341094  0.310162  0.561214  0.494855     0.513746  0.342873  0.692227\n",
       "\n",
       "[:, :, 1, 2000] =\n",
       " 0.646456  0.674206  0.661278  0.341869  …  0.448003  0.61391   0.535002\n",
       " 0.675616  0.610764  0.320287  0.574371     0.52798   0.691107  0.45302 \n",
       " 0.398444  0.550841  0.614056  0.512041     0.310639  0.452969  0.438236\n",
       " 0.479359  0.487187  0.647352  0.620346     0.31264   0.485971  0.684974\n",
       " 0.424225  0.5572    0.391602  0.606547     1.37876   0.400834  0.548812\n",
       " 0.654133  0.328372  0.537674  0.365292  …  1.37444   0.583169  0.480093\n",
       " 0.677936  0.436425  0.640999  0.609548     1.5673    0.391656  0.5126  \n",
       " 0.470709  0.385418  0.524139  0.30063      1.24463   0.655763  0.590142\n",
       " 0.666381  0.412392  0.562096  0.323958     1.53784   0.623907  0.490608\n",
       " 0.309145  0.387281  0.419655  0.602253     1.32447   0.597998  0.688471\n",
       " 0.59428   0.462477  0.522457  0.412145  …  0.338829  0.398655  0.367265\n",
       " 0.343631  0.661337  0.414956  0.535907     0.550713  0.512977  0.561777\n",
       " 0.503037  0.544219  0.69521   0.302245     0.514006  0.317261  0.307561\n",
       " ⋮                                       ⋱                              \n",
       " 0.326808  0.548584  0.535143  0.659952     0.451165  0.673764  0.361094\n",
       " 0.544617  0.452091  0.532792  0.514325     0.624108  0.644699  0.477531\n",
       " 0.458713  0.480117  0.321705  0.596149  …  0.382306  0.33005   0.599362\n",
       " 0.347726  0.428313  0.516926  0.370717     0.385891  0.619392  0.329709\n",
       " 0.538739  0.347121  0.647245  0.408146     0.304175  0.533698  0.509726\n",
       " 0.445083  0.615461  0.586274  0.41059      0.533724  0.337719  0.61052 \n",
       " 0.326713  0.599886  0.450992  0.386666     0.448723  0.54202   0.568636\n",
       " 0.500662  0.673441  0.602538  0.609304  …  0.371897  0.422463  0.303604\n",
       " 0.498409  0.399104  0.496413  0.443045     0.53411   0.60956   0.575113\n",
       " 0.352637  0.651941  0.640334  0.61627      0.582497  0.439557  0.495301\n",
       " 0.48041   0.695145  0.425387  0.581645     0.319032  0.488089  0.526198\n",
       " 0.528631  0.331251  0.384771  0.444725     0.623472  0.693309  0.384903"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

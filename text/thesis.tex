% structure as seen in https://www.overleaf.com/learn/latex/How_to_Write_a_Thesis_in_LaTeX_(Part_1):_Basic_Structure

\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\graphicspath{ {images/} }
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsthm}
 
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\title{
{Separation of Foreground and Background Signal in Variational Autoencoders}\\
{\large Albert-Ludwigs-Universit√§t Freiburg}\\[\baselineskip]
{\includegraphics[width=7cm]{logo.png}}
}
\author{Florian Eichin}
\date{02.12.1995}

\begin{document}

\maketitle

\chapter*{Abstract}
Abstract goes here
 
\chapter*{Dedication}
To mum and dad
 
\chapter*{Declaration}
I declare that..
 
\chapter*{Acknowledgements}
I want to thank...
 
\tableofcontents

\chapter{Introduction}

\chapter{Technical Part}
%TODO Zusammenfassung

For this purpose, many ideas of the following chapter are taken from (XX) and the notation mainly follows the same logic. %TODO explain notation + continuous/discrete

\section{Inference Problem Setup}
%TODO : Rewrite intro, generative model
Before we dive into the technical questions of this thesis, we want to begin with a discussion of the problem we attempt to solve formally. Let $\mathbf{x}, \mathbf{z}$ be random variables with $\mathbf{x}$ observable and $\mathbf{z} \in \mathbb{R}^k$ hidden. Then we are interested in the \textit{latent variable model} with model parameters $\theta^*$
\begin{equation}
	p_{\mathbf{\theta^*}}(\mathbf{x}, \mathbf{z}) = p_{\mathbf{\theta^*}}(\mathbf{x}| \mathbf{z})p_{\mathbf{\theta^*}}(\mathbf{z})
\end{equation}
We further assume that prior $p_{\mathbf{\theta^*}}(\mathbf{z})$ and $p_{\mathbf{\theta^*}}(\mathbf{x}|\mathbf{z})$ are from parametric families of distributions $p_{\mathbf{\theta}}(\mathbf{z})$ and $p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z})$ and that they have probability density functions that are differantiable with respect to $\mathbf{\theta}$ and $\mathbf{z}$ almost everywhere.

To make things clearer, we can  look at it in a more practical way: Let $\mathbf{X} = \{ \mathbf{x}^{(i)}\}_{i=1}^N$ be a dataset with $N$ i.i.d. samples of our random variable $\mathbf{x}$. Note, that $\mathbf{x}$ can be a vector of arbitrary dimension encoding all kinds of data such as images, soundwaves etc. If we model our data with the above latent variable model, we suppose the datapoints to be generated with the involvement of $\mathbf{z}$ in the sense, that first a value $\mathbf{z}^{(i)}$ is generated from prior distribution $p_{\mathbf{\theta^*}}(\mathbf{z})$ and in the second step, $\mathbf{x}^{(i)}$ is generated from $p_{\mathbf{\theta^*}}(\mathbf{x}|\mathbf{z^{(i)}})$. 
Usually, $\mathbf{z}$ is assumed to have a much lower dimension and a much simpler distribution than $\mathbf{x}$. Therefore, the $\mathbf{z}$-space can be viewed as a space of encodings, where only relevant information for decoding datapoints into the high-dimensional $\mathbf{x}$-space is retained. From a machine learning perspective, we can identify this model with an dimensionality reduction problem: For our data, we want to learn functions $q_{\phi}: \mathcal{X} \rightarrow \mathcal{Z}$, $p_{\theta}: \mathcal{Z} \rightarrow \mathcal{X}$ such that for our data $\mathbf{X} \approx p_{\theta}(q_{\phi}(\mathbf{X}))$, constrained the low dimension and regularization of $\mathcal(Z)$. In other words, we want an encode our data $\mathbf{X}$ with minimal reconstruction loss when decoding it again and define $q_{\phi}$ and $p_{\theta}$ as encoder and decoder respectively. REWORK


This is interesting for us, as we're not only interested in approximations of the posterior inference of $\mathbf{z}$ given a value of $\mathbf{x}$ but also the ... %TODO

For a given dataset, there is different approaches for the above scenario. However, we do make additional assumptions, that narrow the list of efficient algorithms significantly [XX]:

\begin{itemize}
	\item[1] \emph{Intractability}: the integral of the marginal likelihood $p_{\mathbf{\theta}}(\mathbf{x}) = \int p_{\mathbf{\theta}}(\mathbf{z}) p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z}) d \mathbf{z}$, as well as posterior distribution $p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}) = p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z}) p_{\mathbf{\theta}}(\mathbf{z}) / p_{\mathbf{\theta}}(\mathbf{x})$ are intractable.
	\item[2] \emph{Big dataset}: Batch optimization is too expensive and parameter updates on small minibatches preferable. Sampling-based solutions would be too inefficient [XX].
\end{itemize}

\section{Variational Inference}

In many probabilistic models inference is intractable and approximation methods are needed. One way of approximating solutions to inference problems is to describe it as an optimization problem. Algorithms involving this approach are called \emph{variational}.
Given an intractable probability distribution $p$ and a set of tractable distributions $\mathcal{Q}$, the goal of a variational algorithm is to find $q \in \mathcal{Q}$ that is most 'similar' to $p$. Finding such a $q$ usually involves a complex optimization procedure for an optimization target $J(q)$. Subsequently, we can use $q$ instead of $p$ in order to find approximate solutions to inference problems efficiently.\\
Of course, this rather informal description on variational techniques leaves us with questions. What is the similarity of two distributions $q$ and $p$? How do we choose an according optimization objective $J(q)$? What are good ways of formulating tractable class of distributions $\mathcal{Q}$? \\
The (partial) answering to these four questions will be the main motivation for the following sections in order to lay the groundwork for the introduction of the Variational Autoencoder (VAE). Inheriting it's name from \emph{Autoencoders}[XX], a VAE is a probabilistic model designed for learning latent representations with the help of Artificial Neuronal Networks (ANNs) and a variational optimization approach to the approximation of the encoder distribution.

\section{Kullback-Leibler divergence}
% https://arxiv.org/pdf/1408.4755.pdf
Beggining with our first question, there is a way of quantifying the 'similarity' of two distributions $p(\mathbf{x})$ and $q(\mathbf{x})$ in information theory known as the \emph{Kullback-Leibler (KL) divergence}. For $p(\mathbf{x}), q(\mathbf{x})$ continuous, the KL divergence is defined as %TODO : write as expectation
\begin{equation}
	KL(q(\mathbf{x})||p(\mathbf{x})) = \int_{\mathbf{x}} q(\mathbf{x}) \log \frac{q(\mathbf{x})}{p(\mathbf{x})} d \mathbf{x} = \mathbb{E}_{q(\mathbf{x})}\left[ \log\left(\frac{q(\mathbf{x})}{p(\mathbf{x})} \right) \right]
\end{equation}
In the discrete case, it is analogously 
\begin{equation}
	KL(q(\mathbf{x})||p(\mathbf{x})) = \sum_{\mathbf{x}} q(\mathbf{x}) \log \frac{q(\mathbf{x})}{p(\mathbf{x})} = \mathbb{E}_{q(\mathbf{x})}\left[ \log\left(\frac{q(\mathbf{x})}{p(\mathbf{x})} \right) \right]
\end{equation}
Here, $\log$ is an abbreviation for the logarithm to base $2$. Note, that for any $q(\mathbf{x}), p(\mathbf{x})$ (continuous or discrete) we can deduce the following properties:
\begin{itemize}
	\item $KL(q||p) \geq 0$
	\item $KL(q||p) = 0$ if and only if $q = p$ (QUESTION: Do we need this???)
\end{itemize}
For a proof, we can consider $q(\mathbf{x}), p(\mathbf{x})$ to be continuous (the discrete case follows analofgously). With $1 - r \leq -\log(r)$ we have:
\begin{equation}
\begin{split}
KL(q(\mathbf{x})||p(\mathbf{x})) 
& = \int_{\mathbf{x}} q(\mathbf{x}) \log \frac{q(\mathbf{x})}{p(\mathbf{x})} d \mathbf{x} \\
& = \int_{\mathbf{x}} q(\mathbf{x}) (- \log \frac{p(\mathbf{x})}{q(\mathbf{x})}) d \mathbf{x} \\
& \geq \int_{\mathbf{x}} q(\mathbf{x}) (1 - \frac{p(\mathbf{x})}{q(\mathbf{x})}) d \mathbf{x} \\
& = \int_{\mathbf{x}} q(\mathbf{x}) d \mathbf{x} - \int_{\mathbf{x}} q(\mathbf{x}) \frac{p(\mathbf{x})}{q(\mathbf{x})} d \mathbf{x} = 0 
\end{split}
\end{equation}
And '$=$' $ \Leftrightarrow \frac{p(\mathbf{x})}{q(\mathbf{x})} = 1 \Leftrightarrow p(\mathbf{x})=q(\mathbf{x})$.

Before we dive deeper into how to utilize the KL divergence for our problem, let us gain a better understanding of why it is a sound choice for measuring 'similarity'. %TODO

\section{Variational Lower Bound (ELBO)}
%TODO : motivation of maximization of px
As we discussed previously, $p_{\mathbf{\theta}}(\mathbf{x})$ as well as $p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x})$ are supposed to be intractable in our problem setup. We have thus no way of retrieving either of the two out of the other. This is where the variational component from two sections before comes into play. In order to approximate $p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x})$ we introduce a tractable \emph{parametric inference model} $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$. We will optimize the so called \emph{variational parameters} $\pmb{\phi}$ of this model such that $p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}) \approx q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$.
Derived from Bayes' rule, we also have
\begin{equation}
	p_{\mathbf{\theta}}(\mathbf{x}) = \frac{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}) p_{\mathbf{\theta}}(\mathbf{z})}{p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z})} \approx  \frac{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}) p_{\mathbf{\theta}}(\mathbf{z})}{q_{\mathbf{\phi}}(\mathbf{x}|\mathbf{z})}
\end{equation}

It is clear, that for our model to fit the true distribution of our data well, we are interested in the following two things:
\begin{itemize}
	\item[1.] Maximization of the marginal likelihood $p_{\mathbf{\theta}}(\mathbf{x})$ for our data to improve our generative model.
	\item[2.] Minimization of the KL divergence between $p_{\mathbf{\theta}}(\mathbf{x})$ and $q_{\mathbf{\phi}}(\mathbf{x})$ to improve the approximation of $q_{\mathbf{\phi}}(\mathbf{x})$.
\end{itemize}

Since $\log$ is monotonous, maximizing $p_{\mathbf{\theta}}(\mathbf{x})$ is equivalent to maximizing $\log p_{\mathbf{\theta}}(\mathbf{x})$. For an arbitrary choice of $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$ we can consider the following derivation:
\begin{equation}
\begin{split}
	\log p_{\mathbf{\theta}}(\mathbf{x}) 
	& = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[\log p_{\mathbf{\theta}}(\mathbf{x})\right] \\
	& = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log \frac{p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x})} \right] \\
	& = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log\left(\frac{p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\frac{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x})} \right) \right] \\
	& = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log\left(\frac{p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\right) \right] + \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log\left(\frac{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x})} \right) \right] \\
	& = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log\left(\frac{p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\right) \right] + KL(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) || p_{\mathbf{\theta}}(\mathbf{z}| \mathbf{x})) 
\end{split}
\end{equation}
Where the right term in the last row is the KL divergence of $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$ and $p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})$. If we rearrange the equation, we have the following:
\begin{equation}
	\log p_{\mathbf{\theta}}(\mathbf{x}) - KL(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) || p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})) = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log\left(\frac{p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\right) \right]
\end{equation}
	And since $KL(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) || p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})) \geq 0$, the right hand side is a lower bound for $\log p_{\mathbf{\theta}}(\mathbf{x})$. It is also referred to as \emph{variational lower bound} or \emph{evidence lower bound} (ELBO):
\begin{equation}
\begin{split}
	\mathcal{L}_{\mathbf{\theta}, \mathbf{\phi}}(\mathbf{x}) 
	& = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log\left(\frac{p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\right) \right] \\
	& = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) - \log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) \right]	\\
	% & = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) \right] - \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[\log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) \right] \\
	% & = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) \right] - KL()
\end{split}
\end{equation}
With the above derivation in mind, we can identify another interpretation of $KL(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) || p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}))$ besides being the KL divergence of approximate posterior $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$ and true posterior $p_{\mathbf{\theta}}(\mathbf{x}| \mathbf{z}))$: It is also the gap between ELBO $\mathcal{L}_{\mathbf{\theta}, \mathbf{\phi}}(\mathbf{x})$ and $\log p_{\mathbf{\theta}}(\mathbf{x})$. If $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$ approximates the true $p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x})$ 'better', the gap gets smaller.

Let $\mathbf{X}$ be the dataset of i.i.d. samples from before and $N_{\mathbf{X}} = |\mathbf{X}|$. If we want to fit our model on $\mathbf{X}$, the ELBO yields us an optimization objective we were asking for, namely the average of ELBOs of single datapoints $\mathbf{x} \in \mathbf{X}$:
\begin{equation}
	\mathcal{L}_{\mathbf{\theta}, \mathbf{\phi}}(\mathbf{X}) = \sum_{\mathbf{x} \in \mathbf{X}} \frac{\mathcal{L}_{\mathbf{\theta}, \mathbf{\phi}}(\mathbf{x})}{N_{\mathbf{X}}}
\end{equation}
If we maximize $\mathcal{L}_{\mathbf{\theta}, \mathbf{\phi}}(\mathbf{x})$ with respect to parameters $\pmb{\theta}$ and $\pmb{\phi}$ for our data, we will approximately maximize $p_{\mathbf{\theta}}(\mathbf{x})$ and minimize $KL(p_{\mathbf{\theta}}(\mathbf{x}) || q_{\mathbf{\phi}}(\mathbf{x}))$ just like the goals we formulated in the beginning of this section.

\section{Auto-encoding Variational Bayes}
\subsection{Batch Gradient Descent}
%TODO
With the means of the ELBO, we now have an objective to optimize the model parameters $\pmb{\theta}$ and $\pmb{\phi}$ for. A naive solution, also known as \emph{Batch Gradient Descent}, is to initialize the parameters randomly, to estimate the gradients $\nabla_{\pmb{\theta}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{X})$ and $\nabla_{\pmb{\pmb{\phi}}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{X})$ and adjust $\pmb{\theta}$ and $\pmb{\phi}$ into their respective directions until convergence. With each step of adjusting the parameters for the gradients, also called an \emph{epoch}, we expect $\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{X})$ to improve until we have reached a local maximum and the algorithm converges. It is up to implementation how to detect convergence of the algorithm. Typically, one can define criteria such as a threshold for change of loss after a certain number of epochs epoch. If the change of loss is lower than this set threshhold, we abort the procedure. Of course, there is other, more complex criteria. Sometimes the user decides theirselves, when to stop the algorithm due to the tradeoff between runtime and optimality. A further discussion can be found in [XX].
\begin{algorithm}[H]
\SetAlgoLined
\While{not converged}{
Estimate gradients $\nabla_{\pmb{\theta}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{M})$, $\nabla_{\pmb{\pmb{\phi}}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{M})$\\
Update parameters $\pmb{\theta} \rightarrow \pmb{\theta} + \eta \nabla_{\pmb{\theta}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x})$, $\pmb{\phi} \rightarrow \pmb{\phi} + \eta \nabla_{\pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x})$
}
\caption{Batch Gradient Descent}
\end{algorithm} 
Note, that $\eta$ is a hyperparameter, that determines the extent to which the gradients update the parameters for each epoch. It is therefore also called the \emph{learning rate} and is an important parameter to choose as it highly affects the convergence of the algorithm: If we choose $\eta$ too small, we will only move slowly in the 'preferred' direction of local maxima. However, if we choose $\eta$ too big, our updating-steps get too large and the algorithm cannot converge in the worst case. 


\subsection{Estimation of the gradients}
Because in general analytical computations of the gradients of the ELBO are intractable, we have to estimate them. For $\nabla_{\pmb{\theta}, \pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{X})$, it is sufficient to estimate $\nabla_{\pmb{\theta}, \pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x})$ for $\mathbf{x} \in \mathbf{X}$ because since (XX) we have
\begin{equation}
	\nabla_{\pmb{\theta}, \pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{X}) = \nabla_{\pmb{\theta}, \pmb{\phi}}  \sum_{\mathbf{x} \in \mathbf{X}} \frac{\mathcal{L}_{\mathbf{\theta}, \mathbf{\phi}}(\mathbf{x})}{N_{\mathbf{X}}} = \frac{1}{N_{\mathbf{X}}} \sum_{\mathbf{x} \in \mathbf{X}} \nabla_{\pmb{\theta}, \pmb{\phi}} \mathcal{L}_{\mathbf{\theta}, \mathbf{\phi}}(\mathbf{x})
\end{equation}
For gradients of the ELBO with respect to generative model parameters $\pmb{\theta}$, it is simple to obtain an unbiased Monte Carlo estimator:
\begin{equation}
\begin{split}
\nabla_{\pmb{\theta}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x}) 
& = \nabla_{\pmb{\theta}} \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) - \log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) \right]	\\
& = \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \nabla_{\pmb{\theta}} (\log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) - \log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})) \right] \\
& \simeq \nabla_{\pmb{\theta}} (\log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) - \log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})) \\
& = \nabla_{\pmb{\theta}} \log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z})\\
\end{split}
\end{equation}
With the $\mathbf{z}$ in the last two lines being a random sample from $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$. For unbiased gradients with respect to $\pmb{\phi}$, things are a bit more difficult. This is due to the fact, that in general, we have:
\begin{equation}
\begin{split}
\nabla_{\pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x}) 
& = \nabla_{\pmb{\phi}} \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) - \log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) \right]	\\
& \neq \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \nabla_{\pmb{\phi}} (\log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) - \log q_{\pmb{\phi}}(\mathbf{z}|\mathbf{x})) \right] \\
\end{split}
\end{equation}
%TODO : check argument for continuous and diff
Moreover, we will thus assume $\mathbf{z}$ to be continuous and $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$ and $p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z})$ differentiable. For the case of the Variational Autoencoder (and other instances of our problem), we are not constrained by this. However, we can now reparameterize $\mathbf{z} \sim q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$ to circumvent the problem of obtaining $\nabla_{\pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x})$. We choose $f$ as some differentiable transformation and introduce another random variable $\pmb{\epsilon}$ independent of $\pmb{\phi}$, $\pmb{\theta}$ and $\mathbf{x}$ such that
\begin{equation}
\mathbf{z} = f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})
\end{equation}
This is also referred to as \emph{reparameterization trick}. With (2.12) we can rewrite our gradient as follows:
\begin{equation}
\begin{split}
\nabla_{\pmb{\theta}, \pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x}) 
& = \nabla_{\pmb{\theta}, \pmb{\phi}} \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[ \log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) - \log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) \right]	\\
& = \nabla_{\pmb{\theta}, \pmb{\phi}} \mathbb{E}_{p(\pmb{\epsilon})}\left[ \log p_{\mathbf{\theta}}(\mathbf{x}, f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})) - \log q_{\mathbf{\phi}}(f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})|\mathbf{x}) \right]	\\
& = \mathbb{E}_{p(\pmb{\epsilon})}\left[ \nabla_{\pmb{\theta}, \pmb{\phi}}(\log p_{\mathbf{\theta}}(\mathbf{x}, f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})) - \log q_{\mathbf{\phi}}(f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})|\mathbf{x})) \right]	\\
& \simeq \nabla_{\pmb{\theta}, \pmb{\phi}}(\log p_{\mathbf{\theta}}(\mathbf{x}, f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})) - \log q_{\mathbf{\phi}}(f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})|\mathbf{x})) \\
& =: \nabla_{\pmb{\theta}, \pmb{\phi}}\tilde{\mathcal{L}}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x}, \pmb{\epsilon})
\end{split}
\end{equation}
And with $\tilde{\mathcal{L}}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x}, \pmb{\epsilon}) := \log p_{\mathbf{\theta}}(\mathbf{x}, f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})) - \log q_{\mathbf{\phi}}(f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})|\mathbf{x})$ the last row is a Monte Carlo estimator for a single sample $\pmb{\epsilon} \sim p(\pmb{\epsilon})$. 
It is unbiased because:
%TODO : add sums for expectations 
\begin{equation}
\begin{split}
\mathbb{E}_{p(\pmb{\epsilon})}\left[ \nabla_{\pmb{\theta}, \pmb{\phi}}\tilde{\mathcal{L}}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x}, \pmb{\epsilon}) \right] 
& = \mathbb{E}_{p(\pmb{\epsilon})}\left[ \nabla_{\pmb{\phi}, \pmb{\theta}} \log p_{\mathbf{\theta}}(\mathbf{x}, f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})) - \log q_{\mathbf{\phi}}(f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})|\mathbf{x}) \right] \\
& = \nabla_{\pmb{\phi}, \pmb{\theta}} \mathbb{E}_{p(\pmb{\epsilon})}\left[  \log p_{\mathbf{\theta}}(\mathbf{x}, f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})) - \log q_{\mathbf{\phi}}(f(\pmb{\epsilon}, \pmb{\phi}, \mathbf{x})|\mathbf{x}) \right] \\
& = \nabla_{\pmb{\phi}, \pmb{\theta}} \mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})}\left[  \log p_{\mathbf{\theta}}(\mathbf{x}, \mathbf{z}) - \log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) \right] \\
& = \nabla_{\pmb{\theta}, \pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x}) 
\end{split}
\end{equation}
Also, $\tilde{\mathcal{L}}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x}, \pmb{\epsilon})$ is an unbiased estimator for $\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{x})$ with an analog argument.


\subsection{Stochastic Gradient Descent} 
Vanilla Gradient Descent has several shortcomings, that we still have to address. For one, it is usually very expensive to estimate $\nabla_{\pmb{\theta}, \pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{X})$ on big datasets $\mathbf{X}$, violating our goal for efficiency with large data. Also, the algorithm usually only finds the closest local maximum and has no means to escape before convergence. There is several solutions to those problems. One that tackles both is called \emph{Minibatch Stochastic Gradient Descent (SGD)}. 
The idea behind Minibatch SGD is again intuitive: Instead of estimating the gradients on the whole dataset $\mathbf{X}$, we randomly draw a subset $\mathbf{M} \subset \mathbf{X}$ of size $N_{\mathbf{M}}$. We call $\mathbf{M}$ a \emph{minibatch}. 
With $\pmb{\alpha} \in \{\pmb{\theta}, \pmb{\phi}\}$ we have:
\begin{equation}
\nabla_{\pmb{\alpha}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{X}) 
= \frac{1}{N_{\mathbf{X}}} \sum_{\mathbf{x} \in \mathbf{X}} \nabla_{\pmb{\alpha}} \mathcal{L}_{\mathbf{\theta}, \mathbf{\phi}}(\mathbf{x}) 
\simeq \frac{1}{N_{\mathbf{M}}} \sum_{\mathbf{x} \in \mathbf{M}} \nabla_{\pmb{\alpha}} \mathcal{L}_{\mathbf{\theta}, \mathbf{\phi}}(\mathbf{x})
= \nabla_{\pmb{\alpha}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{M}) 
\end{equation}
Assume $\mathbf{M}'=\{(\mathbf{x}, \pmb{\epsilon}) |  \mathbf{x} \in \mathbf{M}, \pmb{\epsilon} \sim p(\pmb{\epsilon})\}$ to be a set of tuples of each datapoint in $\mathbf{M}$ and an according sample of $\pmb{\epsilon}$. For $\pmb{\alpha} = \pmb{\theta}$, we can get the following unbiased gradient estimator from combining (2.10) with (2.12):
\begin{equation}
	\nabla_{\pmb{\theta}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{M}) 
	\simeq \frac{1}{N_{\mathbf{M}}} \sum_{(\mathbf{x}^{(i)}, \pmb{\epsilon}^{(i)}) \in \mathbf{M}'} \nabla_{\pmb{\theta}} \log p_{\mathbf{\theta}}(\mathbf{x}, f(\pmb{\epsilon}^{(i)}, \pmb{\phi}, \mathbf{x})) \\
\end{equation}
And for $\pmb{\alpha} = \pmb{\phi}$ with (2.13) we can derive:
\begin{equation}
\nabla_{\pmb{\phi}}\mathcal{L}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{M}) 
	\simeq \frac{1}{N_{\mathbf{M}}} \sum_{(\mathbf{x}^{(i)}, \pmb{\epsilon}^{(i)}) \in \mathbf{M}'} \nabla_{\pmb{\phi}}(\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}, f(\pmb{\epsilon}^{(i)}, \pmb{\phi}, \mathbf{x})) - \log q_{\mathbf{\phi}}(f(\pmb{\epsilon}^{(i)}, \pmb{\phi}, \mathbf{x}^{(i)})|\mathbf{x}^{(i)}))
\end{equation}
Usually the minibatch size $N_{\mathbf{M}}$ is set to be much lower than the number of datapoints in our dataset. Depending on application domain and optimization target, different sizes are optimal. For $N_\mathbf{M}=1$ we also have 'normal' \emph{Stochastic Gradient Descent} as an instance of Minibatch SGD. It is easy to see, why Minibatch SGD is a more efficient way of optimizing our model parameters: The cost of estimating our gradients on smaller minibatches is of course much cheaper than on the whole dataset. In the same runtime, Minibatch SGD updates parameters much more often than vanilla Gradient Descent, usually leading to much faster convergence. Besides bare runtime, it also saves memory as we only have to load a small fraction of the dataset in order for the algorithm to work. Also, because we update our parameters on random subsets, the optimization process becomes more 'noisy', allowing the algorithm to escape local maxima by taking globally non-optimal steps. However, this does not solve all the problems with local maxima. Theres more techniques to tackle this problem, but an in-depth discussion would by far escape the scope of this thesis. The interested reader is advised to continue their research in [XX].


%TODO : maybe complexity?

\subsection{Stochastic Optimization of the ELBO}
Putting things together, we can now introduce the \emph{Auto-Encoding Variational Bayes} procedure, that utilizes Minibatch SGD and the gradient estimators we have derived.

\begin{algorithm}[H]
\SetAlgoLined
Initialize parameters $\pmb{\theta}$, $\pmb{\phi}$ randomly\\
\While{not converged}{
Randomly draw a minibatch $\mathbf{M} \subset \mathbf{X}$\\
Sample $\pmb{\epsilon} \sim p(\pmb{\epsilon})$\\
Compute $\tilde{\mathcal{L}}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{M})$ and estimate gradient $\nabla_{\pmb{\theta}, \pmb{\phi}}\tilde{\mathcal{L}}_{\pmb{\theta}, \pmb{\phi}}(\mathbf{M})$\\
Update parameters $\pmb{\theta}$, $\pmb{\phi}$
}\caption{Auto-Encoding Variational Bayes (AEVB)}
\end{algorithm}
Note, how not only the minibatch drawing of SGD, but also the sampling of $\epsilon \sim p(\epsilon)$ introduces noise to the procedure. This makes it even more robust to getting stuck in local maxima, for the reasosns discussed above. Also, we have replaced the parameter updating in order to account for more complex strategies of different instances of this algorithm. While simply updating the parameters in the direction of the gradients with a set learning rate $\eta$ is still a viable approach, there is other variations that can lead to better results for different application domains. A more detailed discussion can be found in [XX].


\section{The Variational Auto-encoder (VAE)}
\subsection{Choice of model}

The definition of the AEVB algorithm left some options regarding the exact nature of the final model. Namely, we still have to choose:
\begin{itemize}
\item[1)] the parameterization of decoder model $p_{\pmb{\theta}}(\mathbf{x}|\mathbf{z})$
\item[2)] the parameterization for the variational encoder model $q_{\pmb{\phi}}(\mathbf{z}|\mathbf{x})$
\end{itemize}
We want 1) to be complex enough to model the data sufficiently well. 2) brings us back to our last question from the beginning of this chapter, where we asked for a good way to formulate a the variational class of tractable distributions. As announced before, the Variational Autoencoder uses different Artificial Neural N\\

\subsection{Artificial Neural Networks}
As announced before, the Variational Autoencoder answers this question with a very popular class of algorithms called \emph{Artificial Neural Networks} (ANN), that have gained a lot of attention due to their stellar performance in many different domains of machine learning lately. An ANN is a function composed by \emph{layers} $\alpha^{(i)}$: 
\begin{equation}
NeuralNet = \alpha^{(L)} \circ \alpha^{(L-1)} \circ ... \circ \alpha^{(1)}
\end{equation}
With $L$ the number of layers. For each $i \in \{2, ..., L\} $ we define \emph{weight matrix} $W^{(i)} \in Mat(M^{(i)} \times N^{(i)}, \mathbb{R})$, \emph{bias vector} $b^{(i)} \in \mathbb{R}^{M^{(i)}}$ and \emph{activation function} $g^{(i)}: \mathbb{R} \rightarrow \mathbb{R}$. Layer $\alpha^{(i)}$ maps a vector of \emph{input dimension} $N^{(i)}$ to a vector of \emph{output dimension} $N^{(i+1)}$ as follows:
\begin{equation}
\begin{split}
\alpha^{(i)}: \mathbb{R}^{N^{(i)}} & \rightarrow \mathbb{R}^{N^{(i+1)}} \\
x & \mapsto g^{(i)}(W^{(i)}x+b^{(i)})\\
\end{split}
\end{equation}
These are also called the \emph{hidden layers} of our network and $\alpha^{(L)}$ is the \emph{output layer}. By convention, we define \emph{input layer} $\alpha^{(1)}$ as the identity of the input vector of the network and $N^{(1)}$ as its dimension.
Usually the activation functions $g^{(i)}$, which are applied element-wise, are chosen to be non-linear, or else $\alpha$ would just be a linear function and the ANN would deflate into a single linear transformation as well. 
In order to enable an ANN to 'learn' $\pmb{\phi} = \{W^{(2)},  b^{(2)}, W^{(3)},  b^{(3)}, ..., W^{(L)},  b^{(L)}\}$ are interpreted as parameters, that we can update with respect to their gradients. Therefore, $g^{(i)}$ has to be differentiable (almost) everywhere (we have to define assumed values for the derivation at points where $g^{(i)}$ is non-differntiable). Common activation functions include many others:
\begin{itemize}
\item sigmoid function $Sig(x) = \frac{1}{1 - \exp(-kx)}$ with $k \in \mathbb{R}$
\item rectifier function $ReLu(x) = \max(0, x)$ (with $ReLu'(0) := 0$)
\item tangens hyperbolicus $tanh$
\end{itemize} 
And while each has it's own perks, some are better suited for different tasks. For example, in practice, a big advantage of $ReLu$ is the faster convergence of optimization tasks. On the other hand, $tanh$ and historically important $Sig$ sometimes offer better interpretability of the results as they have values in $(0, 1)$. \\
The gradients with respect to the parameters $\pmb{\phi}$ of a NeuralNetwork can be calculated using a procedure called \emph{Backpropagation}, which exploits the chain rule of differentiation to compute the gradients. Another approach that is currently evolving is \emph{differentiable programming}, where the key idea is to compute derivatives for defined functions automatically. In the case of neural networks, there is only matrix multiplications, vector additions and the usually simple activation functions which makes them a promising application for differential programming. 

\subsection{Neural Networks for parameterizing encoder}
We can use ANNs to parameterize the encoder as a simple factorized Gaussian with mean $\pmb{\mu}$ and log-deviation $\log \pmb{\sigma}$ the output of an ANN with parameters $\pmb{\phi}$ like before:
\begin{equation}
\begin{split}
q_{\pmb{\phi}}(\mathbf{z}|\mathbf{x}) & = \mathcal{N}(\pmb{\mu}, \mathrm{diag}(\pmb{\sigma})^2)(\mathbf{z}) \\
(\pmb{\mu}, \log \pmb{\sigma}) & = NeuralNet_{\pmb{\phi}}(\mathbf{x}) \\
q_{\pmb{\phi}}(\mathbf{z}|\mathbf{x}) & = \prod_i q_{\pmb{\phi}}(\mathbf{z}_i|\mathbf{x}) = \prod_i \mathcal{N}(\pmb{\mu}_i, \pmb{\sigma}_i^2)(\mathbf{z}_i)\\
\end{split}
\end{equation}
Together with the reparameterization trick (XX) %TODO

\subsection{Variational Autoencoder }


\section{Convolutional Neural Networks (CNN)}
While standard, Fully Connected Neuronal networks offer a great way to realize the encoder and decoder of our VAE, there are approaches that work better on certain kinds of data. Convolutional Neural Networks (CNN) are a NN structure, that offers good results when applied on high-dimensional image data. The main idea with CNNs is, that images consists of recurring combinations of shapes that can be learned by lower dimensional filters. %TODO

\appendix
\chapter{Appendix Title}
%\input{chapters/appendix}

\end{document}
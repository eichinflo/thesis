\relax 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{kingma2}
\citation{kingma1}
\citation{kingma2}
\citation{stanford}
\citation{kingma1}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theoretical Part}{6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Inference Problem}{6}\protected@file@percent }
\citation{kingma1}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Variational Inference}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Kullback-Leibler Divergence}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Variational Lower Bound (ELBO)}{9}\protected@file@percent }
\citation{kingma2}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Auto-encoding Variational Bayes}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Batch Gradient Descent}{11}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Batch Gradient Descent}}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Estimation of the Gradients and ELBO}{12}\protected@file@percent }
\citation{kingma1}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Stochastic Gradient Descent}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Stochastic Optimization of the ELBO}{15}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Auto-Encoding Variational Bayes (AEVB)}}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Artificial Neural Networks}{15}\protected@file@percent }
\citation{diff}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}The Variational Auto-encoder (VAE)}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Choice of Model}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Neural Networks for Parameterizing Distributions}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Convolutional Neural Networks (CNN)}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}Convolutions}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.2}Pooling layers}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.3}Utilizing CNNs for Parameterizing the Decoder}{20}\protected@file@percent }
\citation{cvae}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Double ELBO optimization}{21}\protected@file@percent }
\citation{phot}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Implementation and Experiments}{24}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Experiment Design}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Implementation}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Results}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Discussion}{28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Reconstruction plot of eight images $\mathbf  {x}$ sampled from dataset $\mathbf  {X}_{90}$. Each row resembles a different input image and we have columns for the input image $\mathbf  {x}$, output $vae1(\mathbf  {x})$, the scaled input image $g(\mathbf  {x})$ and output $vae2(\mathbf  {x})$.}}{29}\protected@file@percent }
\newlabel{fig:recon1}{{3.1}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Reconstruction plot of eight images $\mathbf  {x}$ sampled from dataset $\mathbf  {X}_{90, 20}$. Each row resembles a different input image and we have columns for the input image $\mathbf  {x}$, output $vae1(\mathbf  {x})$, the scaled input image $g(\mathbf  {x})$ and output $vae2(\mathbf  {x})$.}}{30}\protected@file@percent }
\newlabel{fig:recon3}{{3.2}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Reconstruction plot of eight images $\mathbf  {x}$ sampled from dataset $\mathbf  {X}_{50}$. Each row resembles a different input image and we have columns for the input image $\mathbf  {x}$, output $vae1(\mathbf  {x})$, the scaled input image $g(\mathbf  {x})$ and output $vae2(\mathbf  {x})$.}}{31}\protected@file@percent }
\newlabel{fig:recon2}{{3.3}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Exploration of latent space of a dual VAE trained on dataset $\mathbf  {X}_{90}$. Images show outputs of $vae1$ for different states of $\mathbf  {z}_1$ and $\mathbf  {z}_2$, which are annotated below the respective output in the form [$\mathbf  {z}_{1, 1}$, $\mathbf  {z}_{1, 2}$, $\mathbf  {z}_{1, 3}$, $\mathbf  {z}_{1, 4}$, $\mathbf  {z}_2$].}}{32}\protected@file@percent }
\newlabel{fig:expl1}{{3.4}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Exploration of latent space of a dual VAE trained on dataset $\mathbf  {X}_{90, 20}$. Images show outputs of $vae1$ for different states of $\mathbf  {z}_1$ and $\mathbf  {z}_2$, which are annotated below the respective output in the form [$\mathbf  {z}_{1, 1}$, $\mathbf  {z}_{1, 2}$, $\mathbf  {z}_{1, 3}$, $\mathbf  {z}_{1, 4}$, $\mathbf  {z}_2$].}}{33}\protected@file@percent }
\newlabel{fig:expl3}{{3.5}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Exploration of latent space of a dual VAE trained on dataset $\mathbf  {X}_{50}$. Images show outputs of $vae1$ for different states of $\mathbf  {z}_1$ and $\mathbf  {z}_2$, which are annotated below the respective output in the form [$\mathbf  {z}_{1, 1}$, $\mathbf  {z}_{1, 2}$, $\mathbf  {z}_{1, 3}$, $\mathbf  {z}_{1, 4}$, $\mathbf  {z}_2$].}}{34}\protected@file@percent }
\newlabel{fig:expl2}{{3.6}{34}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusion}{37}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{kingma1}{1}
\bibcite{kingma2}{2}
\bibcite{cvae}{3}
\bibcite{KL}{4}
\bibcite{arithmetic}{5}
\bibcite{diff}{6}
\bibcite{phot}{7}
\bibcite{stanford}{8}

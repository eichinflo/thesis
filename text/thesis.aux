\relax 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Technical Part}{5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Inference Problem Setup}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Variational Inference}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Kullback-Leibler divergence}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Variational Lower Bound (ELBO)}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Auto-encoding Variational Bayes}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Batch Gradient Descent}{10}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Batch Gradient Descent}}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Estimation of the gradients and ELBO}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Stochastic Gradient Descent}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Stochastic Optimization of the ELBO}{14}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Auto-Encoding Variational Bayes (AEVB)}}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Artificial Neural Networks}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}The Variational Auto-encoder (VAE)}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Choice of model}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Neural Networks for parameterizing distributions}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Convolutional Neural Networks (CNN)}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Double ELBO optimization}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Implementation and Experiments}{21}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Implementation}{22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Results}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Discussion}{25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Reconstruction plot of eight images $\mathbf  {x}$ sampled from dataset $\mathbf  {X}_{90}$. Each row resembles a different input image and we have columns for the input image $\mathbf  {x}$, output $vae1(\mathbf  {x})$, the scaled input image $g(\mathbf  {x})$ and output $vae2(\mathbf  {x})$.}}{26}\protected@file@percent }
\newlabel{fig:recon1}{{3.1}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Reconstruction plot of eight images $\mathbf  {x}$ sampled from dataset $\mathbf  {X}_{90, 20}$. Each row resembles a different input image and we have columns for the input image $\mathbf  {x}$, output $vae1(\mathbf  {x})$, the scaled input image $g(\mathbf  {x})$ and output $vae2(\mathbf  {x})$.}}{27}\protected@file@percent }
\newlabel{fig:recon3}{{3.2}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Reconstruction plot of eight images $\mathbf  {x}$ sampled from dataset $\mathbf  {X}_{50}$. Each row resembles a different input image and we have columns for the input image $\mathbf  {x}$, output $vae1(\mathbf  {x})$, the scaled input image $g(\mathbf  {x})$ and output $vae2(\mathbf  {x})$.}}{28}\protected@file@percent }
\newlabel{fig:recon2}{{3.3}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Exploration of latent space of a dual VAE trained on dataset $\mathbf  {X}_{90}$. Images show outputs of $vae1$ for different states of $\mathbf  {z}_1$ and $\mathbf  {z}_2$, which are annotated below the respective output in the form [$\mathbf  {z}_{1, 1}$, $\mathbf  {z}_{1, 2}$, $\mathbf  {z}_{1, 3}$, $\mathbf  {z}_{1, 4}$, $\mathbf  {z}_2$].}}{29}\protected@file@percent }
\newlabel{fig:expl1}{{3.4}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Exploration of latent space of a dual VAE trained on dataset $\mathbf  {X}_{90, 20}$. Images show outputs of $vae1$ for different states of $\mathbf  {z}_1$ and $\mathbf  {z}_2$, which are annotated below the respective output in the form [$\mathbf  {z}_{1, 1}$, $\mathbf  {z}_{1, 2}$, $\mathbf  {z}_{1, 3}$, $\mathbf  {z}_{1, 4}$, $\mathbf  {z}_2$].}}{30}\protected@file@percent }
\newlabel{fig:expl3}{{3.5}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Exploration of latent space of a dual VAE trained on dataset $\mathbf  {X}_{50}$. Images show outputs of $vae1$ for different states of $\mathbf  {z}_1$ and $\mathbf  {z}_2$, which are annotated below the respective output in the form [$\mathbf  {z}_{1, 1}$, $\mathbf  {z}_{1, 2}$, $\mathbf  {z}_{1, 3}$, $\mathbf  {z}_{1, 4}$, $\mathbf  {z}_2$].}}{31}\protected@file@percent }
\newlabel{fig:expl2}{{3.6}{31}}
\bibcite{kingma1}{1}
\bibcite{kingma2}{2}
\bibcite{cvae}{3}
\bibcite{KL}{4}
\bibcite{arithmetic}{5}
\bibcite{stanford}{6}
